{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/oujiayu/anaconda3/envs/myenv/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/oujiayu/anaconda3/envs/myenv/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/oujiayu/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/oujiayu/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/oujiayu/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/oujiayu/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/oujiayu/anaconda3/envs/myenv/lib/python3.10/site-packages (from beautifulsoup4) (2.6)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "圖片 URL: https://s.yimg.com/cv/ae/sports/fonts/2017/Yahoo_Sans-Regular.woff2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "\n",
    "# 設定新聞網站的 URL\n",
    "url = 'https://tw.news.yahoo.com/'\n",
    "# 發送 GET 請求\n",
    "response = requests.get(url)\n",
    "\n",
    "# 確認請求成功\n",
    "if response.status_code == 200:\n",
    "    # 使用 BeautifulSoup 解析 HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# 找到 rel=\"image_src\" 的 <link> 標籤\n",
    "    image_link = soup.find('link', attrs={'rel':['image_src','preload']})\n",
    "    img_tags = soup.find_all('img')\n",
    "\n",
    "    \n",
    "    # 提取 href 屬性中的圖片 URL\n",
    "    if image_link:\n",
    "        img_url = image_link['href']\n",
    "        print(\"圖片 URL:\", img_url)\n",
    "    else:\n",
    "        print(\"未找到圖片\")\n",
    "\n",
    "# 遍歷所有 <img> 標籤\n",
    "    for img in img＿tags:\n",
    "        src = img.get('src')\n",
    "        width = img.get('width')\n",
    "        height = img.get('height')\n",
    "        style = img.get('style')\n",
    "\n",
    "        # 過濾掉 1x1 大小的圖片\n",
    "        if width == \"1\" and height == \"1\":\n",
    "            continue\n",
    "\n",
    "        # 過濾掉隱藏到頁面之外的圖片\n",
    "        if style and \"position:absolute\" in style and \"top:-9999px\" in style and \"left:-9999px\" in style:\n",
    "            continue\n",
    "\n",
    "        # 過濾掉來自特定廣告或追蹤域名的圖片\n",
    "        if src and any(domain in src for domain in [\"doubleclick.net\", \"toast.com\", \"gssprt.jp\"]):\n",
    "            continue\n",
    "\n",
    "        # 如果圖片不是追蹤像素，繼續處理\n",
    "        urls = urljoin(url, src)\n",
    "        #print(urls)\n",
    "    \n",
    "        # 定義一個正則表達式來匹配有效的圖片URL\n",
    "        pattern = re.compile(r\"https:\\/\\/s\\.yimg\\.com\\/ny\\/api\\/res\\/1\\.2\\/.*\\/YXBwaWQ9aGlnaGxhbmRlcjt3=.*\\/creatr-uploaded-images\\/\")\n",
    "\n",
    "        # 過濾出有效的圖片URL\n",
    "        valid_urls = [url for url in urls if pattern.match(url)]\n",
    "\n",
    "        # 輸出有效圖片URL\n",
    "        for valid_url in valid_urls:\n",
    "            print(valid_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<link href=\"https://img.ltn.com.tw/Upload/market/page/2024/10/18/th-241018-16659-1-WdH6U.jpg\" rel=\"image_src\" type=\"image/jpeg\"/>\n",
      "['https://ad.doubleclick.net/ddm/activity/src=9530821;type=invmedia;cat=ltn_w0;u1=market;u2=Money夯話題;u3=16659;u4=;u5=;dc_lat=;dc_rdid=;tag_for_child_directed_treatment=;tfua=;npa=;ord=1?', 'https://cache.ltn.com.tw/images/rwd_ltnlogo.png', 'https://cache.ltn.com.tw/images/logo_foot.png', 'https://cache.ltn.com.tw/images/new.gif', 'https://cache.ltn.com.tw/images/weather/wi_0002_day.png', 'assets/images/default.jpg', 'assets/images/default.jpg', 'assets/images/default.jpg', 'assets/images/default.jpg', 'assets/images/default.jpg', 'assets/images/default.jpg', 'assets/images/default.jpg', 'assets/images/default.jpg', 'assets/images/default.jpg', 'assets/images/default.jpg', 'assets/images/default.jpg', 'assets/images/default.jpg', 'assets/images/default.jpg', 'assets/images/default.jpg', 'https://cache.ltn.com.tw/images/appstore.png', 'https://cache.ltn.com.tw/images/googleplay.png', 'https://img.ltn.com.tw/Upload/market/page/2024/10/18/th-241018-16659-1-WdH6U.jpg']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def download_images(url):\n",
    "    response = requests.get(url)\n",
    "    #print(response.text)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    '''Download images from the given URL and save them to the \"images\" folder.'''\n",
    "    \n",
    "    urls = []\n",
    "    preload_links = soup.find_all('link', rel='preload')\n",
    "    image_link_preload = [link['href'] for link in preload_links if link.get('as') == 'image']\n",
    "    urls.extend(image_link_preload)\n",
    "\n",
    "    # Find all image tags\n",
    "    img_tags = soup.find_all('img')\n",
    "    img_srcs = [img['src'] for img in img_tags if 'src' in img.attrs]\n",
    "    urls.extend(img_srcs)\n",
    "\n",
    "    image_link_image = soup.find('link', rel = 'image_src')\n",
    "    print(image_link_image)\n",
    "    # 提取 href 屬性中的圖片 URL\n",
    "    if image_link_image:\n",
    "        img_url = image_link_image['href']\n",
    "        urls.append(img_url)\n",
    "\n",
    "    print(urls)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = 'https://tw.news.yahoo.com/%E8%A1%8C%E6%94%BF%E5%B7%A5%E4%BD%9C%E5%85%A7%E5%AE%B9%E4%BA%94%E8%8A%B1%E5%85%AB%E9%96%80%E2%8B%AF%E4%BB%96%E5%88%97%E3%80%8C%E4%B8%80%E9%95%B7%E4%B8%B2%E9%A1%8D%E5%A4%96%E4%BB%BB%E5%8B%99%E3%80%8D%E7%B6%B2%E7%A7%92%E6%8E%80%E5%85%B1%E9%B3%B4-063838051.html'\n",
    "    target_url='https://tw.news.yahoo.com/15%E6%AD%B2%E5%B0%91%E5%B9%B4%E7%84%A1%E7%85%A7%E9%A7%95%E9%A7%9B%E6%92%9E%E6%AD%BB3%E4%BA%BA-%E5%A3%AB%E9%99%A2%E5%B0%91%E5%B9%B4%E6%B3%95%E5%BA%AD%E8%A3%81%E5%AE%9A%E6%94%B6%E5%AE%B9-090159741.html'\n",
    "    target_url='https://www.chinatimes.com/realtimenews/20241019001909-260402?chdtv'\n",
    "    target_url='https://www.chinatimes.com/realtimenews/20241019002014-260408?ctrack=pc_main_headl_p01&chdtv'\n",
    "    target_url='https://market.ltn.com.tw/article/16659'\n",
    "    download_images(target_url)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Service\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def download_images(url):\n",
    "    # 設定 ChromeDriver\n",
    "    s = Service('path_to_chromedriver')  # 替換為你的 chromedriver 路徑\n",
    "    driver = webdriver.Chrome(service=s)\n",
    "\n",
    "    # 使用 Selenium 獲取頁面\n",
    "    driver.get(url)\n",
    "\n",
    "    # 獲取頁面 HTML\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # 爬取 <link rel=\"image_src\">\n",
    "    image_link = soup.find('link', rel='image_src')\n",
    "    if image_link:\n",
    "        img_url = image_link['href']\n",
    "        print(f\"圖片 URL: {img_url}\")\n",
    "    else:\n",
    "        print(\"未找到圖片\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    target_url = 'https://www.chinatimes.com/realtimenews/20241019002014-260408?ctrack=pc_main_headl_p01&chdtv'\n",
    "    download_images(target_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "\n",
    "# 設定新聞網站的 URL\n",
    "url = 'https://tw.news.yahoo.com/行政工作內容五花八門⋯他列「一長串額外任務」網秒掀共鳴-063838051.html'\n",
    "\n",
    "# 發送 GET 請求\n",
    "response = requests.get(url)\n",
    "\n",
    "# 確認請求成功\n",
    "if response.status_code == 200:\n",
    "    # 使用 BeautifulSoup 解析 HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 獲取所有 <img> 標籤\n",
    "    images = soup.find_all('img')\n",
    "\n",
    "    # 遍歷所有圖片\n",
    "    for img in images:\n",
    "        src = img.get('src')\n",
    "        width = img.get('width')\n",
    "        height = img.get('height')\n",
    "        style = img.get('style')\n",
    "\n",
    "        # 過濾掉 1x1 大小的圖片\n",
    "        if width == \"1\" and height == \"1\":\n",
    "            continue\n",
    "\n",
    "        # 過濾掉隱藏到頁面之外的圖片\n",
    "        if style and \"position:absolute\" in style and \"top:-9999px\" in style and \"left:-9999px\" in style:\n",
    "            continue\n",
    "\n",
    "        # 過濾掉來自特定廣告或追蹤域名的圖片\n",
    "        if src and any(domain in src for domain in [\"doubleclick.net\", \"toast.com\", \"gssprt.jp\"]):\n",
    "            continue\n",
    "\n",
    "        # 生成完整的圖片 URL\n",
    "        full_url = urljoin(url, src)\n",
    "\n",
    "        # 定義一個正則表達式來匹配有效的圖片URL\n",
    "        pattern = re.compile(r\"https:\\/\\/s\\.yimg\\.com\\/ny\\/api\\/res\\/1\\.2\\/.*\\/YXBwaWQ9aGlnaGxhbmRlcjt3=.*\\/creatr-uploaded-images\\/\")\n",
    "\n",
    "        # 如果圖片URL匹配正則表達式，則打印\n",
    "        if pattern.match(full_url):\n",
    "            print(full_url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
